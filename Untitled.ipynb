{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "#demo of beam search for seq2seq model\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    0: 'a',\n",
    "    1: 'b',\n",
    "    2: 'c',\n",
    "    3: 'd',\n",
    "    4: 'e',\n",
    "    5: 'BOS',\n",
    "    6: 'EOS'\n",
    "}\n",
    "reverse_vocab = dict([(v,k) for k,v in vocab.items()])\n",
    "vocab_size = len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def reduce_mul(l):\n",
    "    out = 1.0\n",
    "    for x in l:\n",
    "        out *= x\n",
    "    return out\n",
    "\n",
    "def check_all_done(seqs):\n",
    "    for seq in seqs:\n",
    "        if not seq[-1]:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def decode_step(encoder_context, input_seq):    \n",
    "    #encoder_context contains infortaion of encoder\n",
    "    #ouput_step contains the words' probability\n",
    "    #these two varibles should be generated by seq2seq model\n",
    "    words_prob = [random.random() for _ in range(vocab_size)]\n",
    "    #downvote BOS\n",
    "    words_prob[reverse_vocab['BOS']] = 0.0\n",
    "    words_prob = softmax(words_prob)\n",
    "    ouput_step = [(idx,prob) for idx,prob in enumerate(words_prob)]        \n",
    "    ouput_step = sorted(ouput_step, key=lambda x: x[1], reverse=True)\n",
    "    return ouput_step\n",
    "\n",
    "#seq: [[word,word],[word,word],[word,word]]\n",
    "#output: [[word,word,word],[word,word,word],[word,word,word]]\n",
    "def beam_search_step(encoder_context, top_seqs, k):       \n",
    "    all_seqs = []\n",
    "    for seq in top_seqs:\n",
    "        seq_score = reduce_mul([_score for _,_score in seq])\n",
    "        if seq[-1][0] == reverse_vocab['EOS']:\n",
    "            all_seqs.append((seq, seq_score, True))\n",
    "            continue\n",
    "        #get current step using encoder_context & seq\n",
    "        current_step = decode_step(encoder_context, seq)\n",
    "        for i,word in enumerate(current_step):    \n",
    "            if i >= k:\n",
    "                break\n",
    "            word_index = word[0]\n",
    "            word_score = word[1]   \n",
    "            score = seq_score * word_score\n",
    "            rs_seq = seq + [word]\n",
    "            done = (word_index == reverse_vocab['EOS'])            \n",
    "            all_seqs.append((rs_seq, score, done))            \n",
    "    all_seqs = sorted(all_seqs, key = lambda seq: seq[1], reverse=True)        \n",
    "    topk_seqs = [seq for seq,_,_ in all_seqs[:k]]\n",
    "    all_done = check_all_done(topk_seqs)\n",
    "    return topk_seqs, all_done\n",
    "\n",
    "def beam_search(encoder_context):\n",
    "    beam_size = 3\n",
    "    max_len = 10\n",
    "    #START\n",
    "    top_seqs = [[(reverse_vocab['BOS'],1.0)]]\n",
    "    #loop\n",
    "    for _ in range(max_len):        \n",
    "        top_seqs, all_done = beam_search_step(encoder_context, top_seqs, beam_size)\n",
    "        if all_done:            \n",
    "            break        \n",
    "    return top_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path[0]: \n",
      "b(0.2189)\n",
      "\n",
      "\n",
      "Path[1]: \n",
      "EOS(0.2144)\n",
      "\n",
      "\n",
      "Path[2]: \n",
      "c(0.1481)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#encoder_context is not inportant in this demo\n",
    "encoder_context = None\n",
    "top_seqs = beam_search(encoder_context)\n",
    "for i,seq in enumerate(top_seqs):\n",
    "    print('Path[%d]: ' % i)\n",
    "    for word in seq[1:]:\n",
    "        word_index = word[0]\n",
    "        word_prob = word[1]\n",
    "        print('%s(%.4f)' % (vocab[word_index], word_prob),)\n",
    "        if word_index == reverse_vocab['EOS']:\n",
    "            break\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
